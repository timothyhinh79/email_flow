{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "\n",
    "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "# remove unused folders to make it easier to load the data\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick encoder and preprocessing model\n",
    "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1'\n",
    "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Classifier model\n",
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net) # helps prevent overfitting\n",
    "  net = tf.keras.layers.Dense(9, activation='softmax', name='classifier')(net) # 9 categories for financial transactions\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# List files in 'data/train'\n",
    "print(\"Files in 'data/train':\", os.listdir('data/train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training, validation, and test datasets\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 1\n",
    "data = pd.read_csv('data/train/financial_transactions_training_dataset_bert.csv')\n",
    "features = data.drop('category_num', axis=1)\n",
    "labels = data['category_num']\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "features_train, features_temp, labels_train, labels_temp = train_test_split(features, labels, test_size=0.4, random_state=seed)  # 60% training\n",
    "features_val, features_test, labels_val, labels_test = train_test_split(features_temp, labels_temp, test_size=0.5, random_state=seed)  # 20% validation, 20% test\n",
    "\n",
    "# Convert the pandas DataFrames into TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((features_train.values, labels_train.values))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((features_val.values, labels_val.values))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((features_test.values, labels_test.values))\n",
    "\n",
    "# Shuffle and batch the datasets\n",
    "train_batch_size = 1\n",
    "test_batch_size = 1\n",
    "train_dataset = train_dataset.shuffle(len(features_train)).batch(train_batch_size)\n",
    "val_dataset = val_dataset.shuffle(len(features_val)).batch(train_batch_size)\n",
    "test_dataset = test_dataset.batch(test_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer for training\n",
    "epochs = 5\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the classifier model\n",
    "classifier_model = build_classifier_model()\n",
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss='sparse_categorical_crossentropy',\n",
    "                         metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = classifier_model.fit(x=train_dataset,\n",
    "                               validation_data=val_dataset,\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = classifier_model.evaluate(test_dataset)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual and predicted categories for each transaction in the test dataset in a dataframe\n",
    "actual_categories = []\n",
    "predicted_categories = []\n",
    "for text, label in test_dataset:\n",
    "  actual_categories.append(label.numpy()[0])\n",
    "  predicted_categories.append(tf.argmax(classifier_model.predict(text), axis=1).numpy()[0])\n",
    "\n",
    "test_data = pd.DataFrame({'text': features_test['description'], 'actual_category': actual_categories, 'predicted_category': predicted_categories})\n",
    "test_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
